{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf1098b52cbff05",
   "metadata": {},
   "source": [
    "# Robot Simulation with PyCRAM: Object Manipulation and Movement\n",
    "In this notebook, we will walk through a complete example of setting up a robot environment, creating objects, and moving the robot to detect and interact with those objects using **PyCRAM**. We will explain the code step by step and ensure you understand how each function works in the simulation context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa105e4c8a9ebfa",
   "metadata": {},
   "source": [
    "## Step 1: Initialization\n",
    "In this section, we import the necessary libraries for setting up the robot simulation. Each library serves a specific role in controlling the robot, creating the world, and interacting with objects:\n",
    "TFBroadcaster: This is responsible for managing transformations in the environment, enabling the robot to understand spatial relations.\n",
    "VizMarkerPublisher: This is used to visualize markers that help track the robot and object locations within the simulated world.\n",
    "BulletWorld: A class from the PyCRAM framework that helps create and manage the simulation environment.\n",
    "ActionDesignator, LocationDesignator, ObjectDesignator: These represent abstract designators for robot actions, locations, and objects respectively, helping to describe tasks and identify targets.\n",
    "Pose: Represents the position and orientation of objects or robots in the simulated world.\n",
    "SimulatedRobot: This enables running the robot in a simulated mode for testing without requiring a physical robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb69c73b8b92acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.ros.tf_broadcaster import TFBroadcaster\n",
    "from pycram.ros.viz_marker_publisher import VizMarkerPublisher, AxisMarkerPublisher\n",
    "from pycram.worlds.bullet_world import BulletWorld\n",
    "from pycram.designators.action_designator import *\n",
    "from pycram.designators.location_designator import *\n",
    "from pycram.designators.object_designator import *\n",
    "from pycram.datastructures.enums import ObjectType, WorldMode, TorsoState\n",
    "from pycram.datastructures.pose import Pose\n",
    "from pycram.process_module import simulated_robot, with_simulated_robot\n",
    "from pycram.object_descriptors.urdf import ObjectDescription\n",
    "from pycram.world_concepts.world_object import Object\n",
    "from pycram.datastructures.dataclasses import Color\n",
    "\n",
    "extension = ObjectDescription.get_file_extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676b2aabe8b99e",
   "metadata": {},
   "source": [
    "## Step 2: Setting Up the Simulation World\n",
    "In this step, we set up the simulation environment using `BulletWorld`. We are using the `DIRECT` world mode, which means the simulation runs without a GUI (graphical interface), making it faster and more suitable for headless operation. We also initialize a visualization marker publisher to track objects and visualize movements in the simulation.\n",
    "\n",
    "- `BulletWorld(WorldMode.DIRECT)`: Creates a simulation world in headless mode (no visual interface).\n",
    "- `VizMarkerPublisher()`: Helps visualize the movements and positions of objects and robots in the simulation world.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa0922f083b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = BulletWorld(WorldMode.DIRECT)\n",
    "viz = VizMarkerPublisher()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be368b3e82515ca2",
   "metadata": {},
   "source": [
    "## Step 3: Adding the Robot to the World\n",
    "Now, we add the robot into the simulation. In this case, we use a PR2 robot model. We define the robotâ€™s type and position in the world using a pose. The pose is a tuple of (x, y, z) coordinates that specify the robot's starting position.\n",
    "\n",
    "- `Object(robot_name, ObjectType.ROBOT, ...)`: Defines the robot as an object in the world.\n",
    "- `Pose([1, 2, 0])`: Sets the robot's initial position at coordinates (1, 2, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a2fb63a303c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr2 = Object(\"pr2\", ObjectType.ROBOT, \"pr2.urdf\")     \n",
    "apartment = Object('apartment', ObjectType.ENVIRONMENT, f'apartment-small{extension}')\n",
    "milk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.3, 1, 0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335f4f86efd4955",
   "metadata": {},
   "source": [
    "## Step 4: Understanding Action Designator (NavigateAction)\n",
    "Action Designators are high-level descriptions of actions which the robot should execute.\n",
    "\n",
    "Action Designators are created from an Action Designator Description, which describes the type of action as well as the parameter for this action. Parameter are given as a list of possible parameters. For example, if you want to describe the robot moving to a table you would need a NavigateAction and a list of poses that are near the table. The Action Designator Description will then pick one of the poses and return a performable Action Designator which contains the picked pose. A Navigation would look like this:\n",
    "    \n",
    " ```python\n",
    " NavigateAction(target_locations=[Pose([1.7, 2, 0])]).resolve().perform()\n",
    " ```\n",
    "\n",
    "To move the robot we need to create a description and resolve it to an actual Designator. The description of navigation only needs a list of possible poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58567d3c8b2653a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.designators.action_designator import NavigateAction\n",
    "from pycram.datastructures.pose import Pose\n",
    "\n",
    "pose = Pose([1, 0, 0], [0, 0, 0, 1])\n",
    "\n",
    "# This is the Designator Description\n",
    "navigate_description = NavigateAction(target_locations=[pose])\n",
    "\n",
    "# This is the performable Designator\n",
    "navigate_designator = navigate_description.resolve()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0aca864f79d569",
   "metadata": {},
   "source": [
    "What we now did was: create the pose where we want to move the robot, create a description describing a navigation with a list of possible poses (in this case the list contains only one pose) and create an action designator from the description. The action designator contains the pose picked from the list of possible poses and can be performed.\n",
    "\n",
    "\n",
    "Every designator that is performed needs to be in an environment that specifies where to perform the designator either on the real robot or the simulated one. This environment is called simulated_robot similar there is also a real_robot environment.\n",
    "\n",
    "There are also decorators which do the same thing but for whole methods, they are called with_real_robot and with_simulated_robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad245633b44d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.process_module import simulated_robot\n",
    "\n",
    "with simulated_robot:\n",
    "    navigate_designator.perform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e8f591534936e",
   "metadata": {},
   "source": [
    "## Step 5: Let the robot look at the object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346234c812d18c88",
   "metadata": {},
   "source": [
    "The LookAtAction is used to make the robot look at a specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072a22bae564bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.designators.action_designator import LookAtAction\n",
    "from pycram.process_module import simulated_robot\n",
    "from pycram.datastructures.pose import Pose\n",
    "\n",
    "target_location = Pose([1, 0, 0.5], [0, 0, 0, 1])\n",
    "with simulated_robot:\n",
    "    LookAtAction(targets=[target_location]).resolve().perform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4f953470a12ac",
   "metadata": {},
   "source": [
    "## Step 6: Detect the object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c27341851d5a9f",
   "metadata": {},
   "source": [
    "Detect is used to detect objects in the field of vision (FOV) of the robot. . The detect designator will return a resolved instance of an ObjectDesignatorDescription.\n",
    "```python\n",
    "obj_desig = DetectAction(milk_desig).resolve().perform()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b52ce60a091775",
   "metadata": {},
   "source": [
    "## Step 7: Robot Movement and Object Detection\n",
    "In this step, we define how the robot moves through the environment and detects objects. The robot is instructed to move to a target location, look at a specific object (in this case, a milk carton), and detect it. These actions are handled by designators like NavigateAction for moving, LookAtAction for directing its gaze, and DetectAction for identifying objects.\n",
    "\n",
    "NavigateAction: Moves the robot to the specified coordinates.\n",
    "\n",
    "LookAtAction: Rotates the robot to look at the given object or location.\n",
    "\n",
    "DetectAction: Enables the robot to detect and recognize an object based on its type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006c6d01700b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.designators.action_designator import DetectAction, LookAtAction, ParkArmsAction, NavigateAction\n",
    "from pycram.designators.object_designator import BelieveObject\n",
    "from pycram.datastructures.enums import Arms\n",
    "from pycram.process_module import simulated_robot\n",
    "from pycram.datastructures.pose import Pose\n",
    "\n",
    "milk_desig = BelieveObject(names=[\"milk\"])\n",
    "\n",
    "## add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33ad91d7ba778c",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bf0624b55891",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click here to get the solution</summary>\n",
    "\n",
    "```python\n",
    "from pycram.designators.action_designator import DetectAction, LookAtAction, ParkArmsAction, NavigateAction\n",
    "from pycram.designators.object_designator import BelieveObject\n",
    "from pycram.datastructures.enums import Arms\n",
    "from pycram.process_module import simulated_robot\n",
    "from pycram.datastructures.pose import Pose\n",
    "\n",
    "milk_desig = BelieveObject(names=[\"milk\"])\n",
    "\n",
    "with simulated_robot:\n",
    "    ParkArmsAction([Arms.BOTH]).resolve().perform()\n",
    "\n",
    "    NavigateAction([Pose([0, 1, 0], [0, 0, 0, 1])]).resolve().perform()\n",
    "\n",
    "    LookAtAction(targets=[milk_desig.resolve().pose]).resolve().perform()\n",
    "\n",
    "    obj_desig = DetectAction(milk_desig).resolve().perform()\n",
    "\n",
    "    print(obj_desig)\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
