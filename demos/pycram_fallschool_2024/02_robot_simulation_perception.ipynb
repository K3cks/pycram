{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf1098b52cbff05",
   "metadata": {},
   "source": [
    "# Robot Simulation with PyCRAM: Object Manipulation and Movement\n",
    "In this notebook, we will walk through a complete example of setting up a robot environment, creating objects, and moving the robot to detect and interact with those objects using **PyCRAM**. We will explain the code step by step and ensure you understand how each function works in the simulation context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa105e4c8a9ebfa",
   "metadata": {},
   "source": [
    "## Step 1: Initialization\n",
    "\n",
    "We start by importing the necessary libraries to set up the robot simulation. Each library serves a specific role in controlling the robot, creating the world, and interacting with objects:\n",
    "TFBroadcaster: This is responsible for managing transformations in the environment, enabling the robot to understand spatial relations.\n",
    "VizMarkerPublisher: This is used to visualize markers that help track the robot and object locations within the simulated world.\n",
    "BulletWorld: A class from the PyCRAM framework that helps to create and manage the simulation environment.\n",
    "ActionDesignator, LocationDesignator, ObjectDesignator: These represent abstract designators for robot actions, locations, and objects respectively, helping to describe tasks and identify targets.\n",
    "Pose: Represents the position and orientation of objects or robots in the simulated world.\n",
    "SimulatedRobot: This enables running the robot in a simulated mode for testing without requiring a physical robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb69c73b8b92acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.ros.tf_broadcaster import TFBroadcaster\n",
    "from pycram.ros.viz_marker_publisher import VizMarkerPublisher, AxisMarkerPublisher\n",
    "from pycram.worlds.bullet_world import BulletWorld\n",
    "from pycram.designators.action_designator import *\n",
    "from pycram.designators.location_designator import *\n",
    "from pycram.designators.object_designator import *\n",
    "from pycram.datastructures.enums import ObjectType, WorldMode, TorsoState\n",
    "from pycram.datastructures.pose import Pose\n",
    "from pycram.process_module import simulated_robot, with_simulated_robot\n",
    "from pycram.object_descriptors.urdf import ObjectDescription\n",
    "from pycram.world_concepts.world_object import Object\n",
    "from pycram.datastructures.dataclasses import Color\n",
    "from pycram.designators.object_designator import BelieveObject\n",
    "\n",
    "extension = ObjectDescription.get_file_extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676b2aabe8b99e",
   "metadata": {},
   "source": [
    "## Step 2: Setting Up the Simulation World\n",
    "In this step, we set up the simulation environment using `BulletWorld`. We are using the `DIRECT` world mode, which means the simulation runs without a GUI (graphical interface), making it faster and more suitable for headless operation. We also initialize a visualization marker publisher to track objects and visualize movements in the simulation.\n",
    "\n",
    "- `BulletWorld(WorldMode.DIRECT)`: Creates a simulation world in headless mode (no visual interface).\n",
    "- `VizMarkerPublisher()`: Visualizes the movements and positions of objects and robots in the simulation world.\n",
    "TODO: rapid fire simulation robot is teleporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa0922f083b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = BulletWorld(WorldMode.DIRECT)\n",
    "viz = VizMarkerPublisher()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be368b3e82515ca2",
   "metadata": {},
   "source": [
    "## Step 3: Adding the Robot to the World\n",
    "Now, we add the robot into the simulation. In this case, we use a PR2 robot model. We define the robotâ€™s type and position in the world using a pose. The pose is a tuple of (x, y, z) coordinates that specify the robot's starting position.\n",
    "\n",
    "- `Object(robot_name, ObjectType.ROBOT, ...)`: Defines the robot as an object in the world.\n",
    "- `Pose([1, 2, 0])`: Sets the robot's initial position at coordinates (1, 2, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a2fb63a303c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr2 = Object(\"pr2\", ObjectType.ROBOT, \"pr2.urdf\")     \n",
    "apartment = Object('apartment', ObjectType.ENVIRONMENT, f'apartment-small{extension}')\n",
    "milk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.3, 1, 0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335f4f86efd4955",
   "metadata": {},
   "source": [
    "## Step 4: Understanding Action Designator (NavigateAction)\n",
    "Action Designators are high-level descriptions of actions which the robot should execute.\n",
    "\n",
    "Action Designators are created from an Action Designator Description, which describes the type of action as well as the parameter for this action. Parameter are given as a list of possible parameters. For example, if you want to describe the robot moving to a table you would need a NavigateAction and a list of poses that are near the table. The Action Designator Description will then pick one of the poses and return a performable Action Designator which contains the picked pose. A Navigation would look like this:\n",
    "    \n",
    " ```python\n",
    " NavigateAction(target_locations=[Pose([1.7, 2, 0])]).resolve().perform()\n",
    " ```\n",
    "\n",
    "To move the robot we need to create a description and resolve it to an actual Designator. The description of navigation only needs a list of possible poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58567d3c8b2653a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.designators.action_designator import NavigateAction\n",
    "from pycram.datastructures.pose import Pose\n",
    "\n",
    "pose = Pose([1, 0, 0], [0, 0, 0, 1])\n",
    "\n",
    "# This is the Designator Description\n",
    "navigate_description = NavigateAction(target_locations=[pose])\n",
    "\n",
    "# This is the performable Designator\n",
    "navigate_designator = navigate_description.resolve()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0aca864f79d569",
   "metadata": {},
   "source": [
    "What we now did was: \n",
    "  - create the pose where we want to move the robot\n",
    "  - create a description describing a navigation with a list of possible poses (in this case the list contains only one pose) and\n",
    "  - create an action designator from the description\n",
    "\n",
    "The action designator contains the pose picked from the list of possible poses and can then be performed.\n",
    "\n",
    "\n",
    "Every designator that is performed needs to be in an environment that specifies where to perform the designator: Either on the real robot or the simulated one. \n",
    "This environment is called simulated_robot. \n",
    "If we want to perform actions on the real robot we would use real_robot instead.\n",
    "\n",
    "There are also decorators which do the same thing but for whole methods, they are called with_real_robot and with_simulated_robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad245633b44d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.process_module import simulated_robot\n",
    "\n",
    "with simulated_robot:\n",
    "    navigate_designator.perform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e8f591534936e",
   "metadata": {},
   "source": [
    "## Step 5: Let the robot look at the object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346234c812d18c88",
   "metadata": {},
   "source": [
    "The LookAtAction is used to make the robot look at a specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072a22bae564bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.designators.action_designator import LookAtAction\n",
    "from pycram.process_module import simulated_robot\n",
    "from pycram.datastructures.pose import Pose\n",
    "\n",
    "target_location = Pose([1, 0, 0.5], [0, 0, 0, 1])\n",
    "with simulated_robot:\n",
    "    LookAtAction(targets=[target_location]).resolve().perform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4f953470a12ac",
   "metadata": {},
   "source": [
    "## Step 6: Detect the object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c27341851d5a9f",
   "metadata": {},
   "source": [
    "The DetectAction is used to detect objects in the field of vision (FOV) of the robot. The detect designator will return a resolved instance of an ObjectDesignatorDescription.\n",
    "```python\n",
    "obj_desig = DetectAction(milk_desig).resolve().perform()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b52ce60a091775",
   "metadata": {},
   "source": [
    "## Step 7: Robot Movement and Object Detection\n",
    "In this step, we define how the robot moves through the environment and detects objects. The robot is instructed to move to a target location, look at a specific object (in this case, a milk carton), and detect it. These actions are handled by designators like NavigateAction for moving, LookAtAction for directing its gaze, and DetectAction for identifying objects.\n",
    "\n",
    "- NavigateAction: Moves the robot to the specified coordinates.\n",
    "\n",
    "- LookAtAction: Rotates the robot to look at the given object or location.\n",
    "\n",
    "- DetectAction: Enables the robot to detect and recognize an object based on its type.## Believe Object\n",
    "Todo: Make clear that people understand the difference in believe object and object\n",
    "\n",
    "This object designator is used to describe objects that are located in the BulletWorld. So objects that are in the\n",
    "belief state, hence the name. In the future when there is a perception interface, there will be a ```RealObject```\n",
    "description which will be used to describe objects in the real world.\n",
    "\n",
    "Since {meth}`~pycram.designators.object_designator.BelieveObject` describes Objects in the BulletWorld we create a few.\n",
    "\n",
    "```python\n",
    "kitchen = Object(\"kitchen\", ObjectType.ENVIRONMENT, \"kitchen.urdf\")\n",
    "milk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.3, 1, 0.9]))\n",
    "cereal = Object(\"froot_loops\", ObjectType.BREAKFAST_CEREAL, \"breakfast_cereal.stl\", pose=Pose([1.3, 0.9, 0.95]))\n",
    "spoon = Object(\"spoon\", ObjectType.SPOON, \"spoon.stl\", pose=Pose([1.3, 1.1, 0.87]))\n",
    "```\n",
    "\n",
    "Now that we have objects we can create an object designator to describe them. To start off we want an object designator\n",
    "only describing the milk. Since all objects have unique names we can create an object designator using a list with only\n",
    "the name of the object.\n",
    "\n",
    "```python\n",
    "from pycram.designators.object_designator import BelieveObject\n",
    "\n",
    "object_description = BelieveObject(names=[\"milk\"])\n",
    "\n",
    "print(object_description.resolve())\n",
    "```\n",
    "\n",
    "You can also use the type to describe objects, so now we want to have an object designator that describes every food in\n",
    "the world.\n",
    "\n",
    "```python\n",
    "from pycram.designators.object_designator import BelieveObject\n",
    "\n",
    "object_description = BelieveObject(types=[ObjectType.MILK, ObjectType.BREAKFAST_CEREAL])\n",
    "\n",
    "print(object_description.resolve())\n",
    "```\n",
    "So now you can use the robot_desig, apartment_desig and the milk_desig to further programm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006c6d01700b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_desig = BelieveObject(names=[\"milk\"])\n",
    "\n",
    "## add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33ad91d7ba778c",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bf0624b55891",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click here to get the solution</summary>\n",
    "\n",
    "```python\n",
    "from pycram.designators.action_designator import DetectAction, LookAtAction, ParkArmsAction, NavigateAction\n",
    "from pycram.designators.object_designator import BelieveObject\n",
    "from pycram.datastructures.enums import Arms\n",
    "from pycram.process_module import simulated_robot\n",
    "from pycram.datastructures.pose import Pose\n",
    "\n",
    "milk_desig = BelieveObject(names=[\"milk\"])\n",
    "\n",
    "with simulated_robot:\n",
    "    ParkArmsAction([Arms.BOTH]).resolve().perform()\n",
    "\n",
    "    NavigateAction([Pose([0, 1, 0], [0, 0, 0, 1])]).resolve().perform()\n",
    "\n",
    "    LookAtAction(targets=[milk_desig.resolve().pose]).resolve().perform()\n",
    "\n",
    "    obj_desig = DetectAction(milk_desig).resolve().perform()\n",
    "\n",
    "    print(obj_desig)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f353ac3a2cdb549",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 8: Understanding the object designator\n",
    "Object designators are used to describe objects located in the BulletWorld or the real environment and then resolve them during runtime to concrete objects.\n",
    "\n",
    "Object designators are different from the Object class in bullet_world.py in the way that they just describe an object and do not create objects or provide methods to manipulate them. Nevertheless, object designators contain a reference to the BulletWorld object.\n",
    "\n",
    "An Object designator takes two parameters, of which at least one has to be provided. These parameters are:\n",
    "\n",
    "A list of names\n",
    "A list of types\n",
    "Object Designators work similar to Location designators, they get constrains describing a set of objects and when resolved return a specific instance.\n",
    "The robot recognizes the milk carton as an object with the following description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc04c354031d163",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ObjectDesignatorDescription.Object(name=\"milk\", obj_type=ObjectType.MILK, \n",
    "    world_object=Object(id=3, world=<BulletWorld>, name=\"milk\", obj_type=ObjectType.MILK, \n",
    "    color=Color(R=1, G=1, B=1, A=1), \n",
    "    pose=Pose(position=[1.3, 1.0, 0.9], orientation=[0.0, 0.0, 0.0, 1.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee9816eb06fadc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. ID and Saved States\n",
    "ID: 3: The object has an identifier (ID) of 3, which allows it to be uniquely recognized in the world.\n",
    "Saved States: {}: No specific states are saved for this object yet, but this can be useful if the robot needs to remember object states (like previous positions).\n",
    "2. World Information\n",
    "World: The object exists within the simulation's BulletWorld, the environment where all interactions occur.\n",
    "Name: The object is named \"milk\", so the robot knows this is the milk carton.\n",
    "Object Type: The object is of type ObjectType.MILK, telling the robot it's dealing with a milk carton.\n",
    "3. Physical Properties\n",
    "Color: The milk carton is white, represented as Color(R=1, G=1, B=1, A=1) using RGBA values (Red, Green, Blue, Alpha). This helps the robot visually identify it.\n",
    "Pose:\n",
    "Position: The milk is located at coordinates x=1.3, y=1.0, and z=0.9, defining its place in the 3D world.\n",
    "Orientation: The milk has no rotation, with its orientation set to [0.0, 0.0, 0.0, 1.0]. These values define how the object is aligned in the world.\n",
    "4. URDF Model\n",
    "URDF Path: The milk is modeled using a file located at /resources/cached/milk.urdf. URDF files contain the visual and physical properties of objects, allowing the simulation to render the object correctly.\n",
    "5. Frames and Transformations\n",
    "Transformation Frame: The frame used for calculating transformations is set to \"milk\". This frame helps the robot understand where the milk is relative to other objects or itself.\n",
    "Pose Information: The objectâ€™s pose can be retrieved using methods like get_pose(), allowing the robot to track its current location.\n",
    "6. Cache Manager and Attachments\n",
    "Cache Manager: The robot uses caching to improve performance by managing states efficiently.\n",
    "Attachments: There are no other objects attached to the milk. This would be relevant if the object were, for example, inside a drawer or held by a robot arm.\n",
    "\n",
    "One of the great features of PyCRAM is its flexibility in handling both simulated and real environments seamlessly. Regardless of whether we interact with objects in a simulation or the real world, the designator remains the same! In simulation, object detection doesn't rely on a real cameraâ€”instead, the system simulates the detection process. However, when using the same detection action on a real robot, PyCRAM integrates with tools like Robokudo to recognize objects in the physical environment. This capability allows us to easily switch between simulation and reality using the same code, which makes development highly efficient. That said, it's important to remember that the real world is much more complex than the simulation, which is only a simplified model. Nonetheless, the ability to use the same code for both simulated and real robots significantly simplifies transitioning between the two environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34d10c0d06fcc1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 9: Moving the milk Carton inside the fridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3554ae8783cc3ca",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "milk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([1.3, 1, 0.9]))\n",
    "pycram.plan_failures.PerceptionObjectNotFound: Could not find an object with the type ObjectType.MILK in the FOV of the robot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec4e7a4307f3bc",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
