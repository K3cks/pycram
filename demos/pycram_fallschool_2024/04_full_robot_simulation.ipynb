{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e8274f8061eaa9",
   "metadata": {},
   "source": [
    "# Chapter 4: Full Robot Simulation\n",
    "\n",
    "Welcome to the fourth chapter of our hands-on course! In this tutorial, you will focus on creating a complete robot control plan in pycram and gaining an understanding of fundamental concepts in robotic simulation.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this chapter, you should be able to:\n",
    "- Load and initialize a robot and environment for simulation.\n",
    "- Plan and execute a robotic task that includes object detection, grasping, transporting, and placing.\n",
    "- Understand key concepts like coordinate transformations, pose calculations, and error handling.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f22a9fe1531dc2",
   "metadata": {},
   "source": [
    "## Step 1: Getting Started\n",
    "\n",
    "In this step, we’ll set up the environment and load all necessary libraries. We will also initialize the simulation, creating a robot in a kitchen setting.\n",
    "\n",
    "*Objective:* By the end of this step, you should have a fully initialized simulation environment and robot to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8cd95305d1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycram.external_interfaces.ik import request_ik\n",
    "from pycram.plan_failures import IKError\n",
    "from pycram.ros.tf_broadcaster import TFBroadcaster\n",
    "from pycram.ros.viz_marker_publisher import VizMarkerPublisher, AxisMarkerPublisher, CostmapPublisher\n",
    "from pycram.utils import _apply_ik\n",
    "from pycram.worlds.bullet_world import BulletWorld\n",
    "from pycram.designators.action_designator import *\n",
    "from pycram.designators.location_designator import *\n",
    "from pycram.designators.object_designator import *\n",
    "from pycram.datastructures.enums import ObjectType, WorldMode, TorsoState\n",
    "from pycram.datastructures.pose import Pose, Transform\n",
    "from pycram.process_module import simulated_robot, with_simulated_robot\n",
    "from pycram.object_descriptors.urdf import ObjectDescription\n",
    "from pycram.world_concepts.world_object import Object\n",
    "from pycram.datastructures.dataclasses import Color\n",
    "\n",
    "extension = ObjectDescription.get_file_extension()\n",
    "\n",
    "world = BulletWorld(WorldMode.DIRECT)\n",
    "world.allow_publish_debug_poses = True\n",
    "viz = VizMarkerPublisher()\n",
    "tf = TFBroadcaster()\n",
    "\n",
    "robot_name = \"pr2\"\n",
    "robot = Object(robot_name, ObjectType.ROBOT, f\"{robot_name}{extension}\", pose=Pose([1, 2, 0]))\n",
    "\n",
    "apartment = Object(\"apartment\", ObjectType.ENVIRONMENT, f\"apartment-small{extension}\")\n",
    "milk = Object(\"milk\", ObjectType.MILK, \"milk.stl\", pose=Pose([0.5, 2.5, 1]))\n",
    "milk.color = Color(0, 0, 1, 1)\n",
    "milk_desig = BelieveObject(names=[\"milk\"])\n",
    "robot_desig = BelieveObject(names=[robot_name])\n",
    "apartment_desig = BelieveObject(names=[\"apartment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ebdb34821e5b5",
   "metadata": {},
   "source": [
    "## Step 2: Detecting the Milk\n",
    "\n",
    " Let's start where we left off with detecting the milk in the open fridge!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15ae72ea3d0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "with simulated_robot:\n",
    "    poseHard = Pose([1.3, 2.7, 0], [0, 0, 1, 0])\n",
    "    NavigateAction([poseHard]).resolve().perform()\n",
    "\n",
    "    ParkArmsAction([Arms.BOTH]).resolve().perform()\n",
    "\n",
    "    MoveTorsoAction([TorsoState.HIGH]).resolve().perform()\n",
    "    handle_desig = ObjectPart(names=[\"handle_cab3_door_top\"], part_of=apartment_desig.resolve())\n",
    "    closed_location, opened_location = AccessingLocation(handle_desig=handle_desig.resolve(),\n",
    "                                                         robot_desig=robot_desig.resolve()).resolve()\n",
    "    OpenAction(object_designator_description=handle_desig, arms=[closed_location.arms[0]],\n",
    "               start_goal_location=[closed_location, opened_location]).resolve().perform()\n",
    "    \n",
    "    LookAtAction(targets=[milk_desig.resolve().pose]).resolve().perform()\n",
    "    obj_desig = DetectAction(milk_desig).resolve().perform()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 3: Task Outline\n",
    "Now, the remaining tasks are: picking up the milk, transporting it, and placing it back near the fridge. While this might seem simple to us, a robot control program must account for many complex details. \n",
    "\n",
    "For a robot to effectively pick up the milk, it needs to understand how to position and move its arm precisely, apply the right amount of force, and assess whether the milk is full, half-full, or empty, as each condition affects the grip strength and energy required. Additionally, it must know if the milk is open or closed, which influences how it should be handled and placed to prevent spills.\n",
    "\n",
    "In this tutorial, we’ll focus on key concepts rather than delving into physical details. The pycram simulation environment, called Bullet World, is designed for rapid testing, allowing the robot to move faster than it would in a real-world setting. To save time, we also teleport the robot instead of having it navigate full paths. This has the advantage that we can quickly write and test plans for the robot.\n",
    "\n",
    "Here’s what we will cover:\n",
    "\n",
    "1. Parking both arms for initial positioning.\n",
    "2. Determining the appropriate grasp based on the object type.\n",
    "3. Finding a reachable pose for the robot to pick up the object with the chosen arm.\n",
    "4. Navigating to the pick-up pose, performing the pick-up, and parking arms afterward.\n",
    "5. Resolving a reachable location for placing the object.\n",
    "6. Navigating to the placement location, performing the placement, and parking arms post-transport.\n",
    "\n",
    "- **Question:** Why do you think it's important for the robot to calculate the approach direction when grasping an object?\n",
    "\n",
    "\n",
    "## Step 4: Calculating Reachable Poses for Grasping\n",
    "\n",
    "Now that we have detected the milk, we need to determine reachable poses for grasping it.\n",
    "\n",
    "*Objective:* By the end of this step, you should be able to calculate a pose for the robot that allows it to reach and grasp the object.\n",
    "\n",
    "\n",
    "#### The Grasp\n",
    "Grasping is a challenging aspect of any robot control program. Depending on where the perception system detects the object, its orientation can vary significantly. Ideally, we want to instruct the robot to \"pick up the milk from the front\" or \"from the top\" in a way that’s easy for humans to specify. For example, regardless of how the milk is positioned in the fridge, the robot should always approach it from the fridge's front. However, if the robot is on the other side of the kitchen, this creates an orientation issue.\n",
    "\n",
    "To address this, we developed the `calculate_object_face` function. This function takes an object as input and, using rotation matrices and the robot's position, calculates the correct orientation to approach the object based on its facing direction relative to the robot.\n",
    "\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click here for the code behind calculate_object:faces</summary>\n",
    "\n",
    "```python\n",
    "def calculate_object_faces(object):\n",
    "    \"\"\"\n",
    "    Calculates the faces of an object relative to the robot based on orientation.\n",
    "\n",
    "    This method determines the face of the object that is directed towards the robot,\n",
    "    as well as the bottom face, by calculating vectors aligned with the robot's negative x-axis\n",
    "    and negative z-axis in the object's frame.\n",
    "\n",
    "    Args:\n",
    "        object (Object): The object whose faces are to be calculated, with an accessible pose attribute.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing two Grasp Enums, where the first element is the face of the object facing the robot,\n",
    "              and the second element is the top or bottom face of the object.\n",
    "    \"\"\"\n",
    "    local_transformer = LocalTransformer()\n",
    "    oTm = object.pose\n",
    "\n",
    "    base_link = RobotDescription.current_robot_description.base_link\n",
    "    marker = AxisMarkerPublisher()\n",
    "    base_link_pose = object.world_object.world.robot.get_link_pose(base_link)\n",
    "\n",
    "    marker.publish([base_link_pose])\n",
    "\n",
    "    oTb = local_transformer.transform_pose(oTm, object.world_object.world.robot.get_link_tf_frame(base_link))\n",
    "    orientation = oTb.orientation_as_list()\n",
    "\n",
    "    rotation_matrix = R.from_quat([orientation[0], orientation[1], orientation[2], orientation[3]]).inv().as_matrix()\n",
    "\n",
    "    robot_negative_x_vector = -rotation_matrix[:, 0]\n",
    "    robot_negative_z_vector = -rotation_matrix[:, 2]\n",
    "\n",
    "    facing_robot_face = calculate_vector_face(robot_negative_x_vector)\n",
    "    bottom_face = calculate_vector_face(robot_negative_z_vector)\n",
    "\n",
    "    return [facing_robot_face, bottom_face]\n",
    "\n",
    "```\n",
    "</details>\n",
    "\n",
    "We’ll have identified the correct grasp based on the object’s orientation. Next, we’ll use `CostmapLocation` to determine a suitable position where the robot can stand to pick up the milk.\n",
    "\n",
    "### Exercise: Finding a Suitable Pose with `CostmapLocation`\n",
    "\n",
    "In this exercise, you’ll use `CostmapLocation` to identify a reachable location where the robot can pick up an object (in this case, the milk) using the correct grasp. Here’s a breakdown of what each parameter in `CostmapLocation` represents:\n",
    "\n",
    "- **`target`**: The designator for the object you want the robot to pick up (e.g., `self.object_designator`).\n",
    "- **`reachable_for`**: The robot or component that needs access to the target (use `robot_desig.resolve()` for the robot).\n",
    "- **`reachable_arm`**: The specific arm that will perform the grasp (e.g., `self.arm`).\n",
    "- **`used_grasps`**: The list of grasps that are suitable for the target object (e.g., `[grasp]`).\n",
    "\n",
    "#### Steps to Complete:\n",
    "\n",
    "1. **Set up the target**: Define the target object (e.g., `object_designator`).\n",
    "2. **Resolve the robot**: Use `robot_desig.resolve()` to specify which robot instance is interacting.\n",
    "3. **Choose an arm**: Select the arm you want to use for this task.\n",
    "4. **Use the determined grasp**: Pass in the appropriate grasp configuration, stored in `grasp = calculate_object_faces(self.object_designator)[0]`.\n",
    "\n",
    "#### Code Template\n",
    "\n",
    "Complete the following code to find a suitable pose with `CostmapLocation`:\n",
    "\n",
    "```python\n",
    "# Define the location using CostmapLocation\n",
    "pickup_loc = CostmapLocation(\n",
    "    target=____,  # Use the object designator\n",
    "    reachable_for=____,  # Resolve the robot designator\n",
    "    reachable_arm=____,  # Specify the arm you are using\n",
    "    used_grasps=____  # Use the determined grasp in a list\n",
    ")"
   ],
   "id": "22addccd35f13baa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c3a59000a5913",
   "metadata": {},
   "outputs": [],
   "source": "## add your code here"
  },
  {
   "cell_type": "markdown",
   "id": "7447f776722e8d0c",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click here for the Solution.</summary>\n",
    "\n",
    "```python\n",
    "with simulated_robot:\n",
    "   grasp = calculate_object_faces(self.object_designator)[0]\n",
    "\n",
    "        pickup_loc = CostmapLocation(\n",
    "            target=self.object_designator,\n",
    "            reachable_for=robot_desig.resolve(),\n",
    "            reachable_arm=self.arm,\n",
    "            used_grasps=[grasp])\n",
    "```\n",
    "</details>\n",
    "You will end up with a list of possible pickup locations. Now, we’ll iterate through these poses to check if any are reachable by the specified arm.\n",
    "\n",
    "```python\n",
    "pickup_pose = next((pose for pose in pickup_loc if self.arm in pose.reachable_arms), None)\n",
    "if not pickup_pose:\n",
    "    raise ObjectUnfetchable(\n",
    "        f\"No reachable pose found for the robot to grasp the object: {self.object_designator} with arm: {self.arm}\"\n",
    "    ) "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once a reachable pose is found, navigate to it and perform a PickUpAction. This action requires the object designator, the selected arm, and the previously determined grasp as parameters.",
   "id": "55eb81d490f6a190"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacb59addf105",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d98dccacc1bdef4",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "\n",
    "<summary>Click here for the Solution.</summary>\n",
    "\n",
    "```python\n",
    "with simulated_robot:\n",
    "\n",
    "    PickUpAction(spoon_desig, [Arms.LEFT], [Grasp.TOP]).resolve().perform()\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "## Step 5: Transport the Object and Final Placement\n",
    "\n",
    "In this step, we’ll use the calculated pose to pick up the milk and place it at the target location.\n",
    "\n",
    "*Objective:* By the end of this step, you will have executed a full pick-and-place operation.\n",
    "\n",
    "\n",
    "### Exercise: Write the plan\n",
    "Your task now is to write a complete plan that includes transporting the object and closing the fridge. The target pose for the milk is:\n",
    "\n",
    "```python\n",
    "milk_target_pose = Pose([5.34, 3.55, 0.8])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f2322ef4fa180",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ab0d9a2fa7df1",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Click here for the Solution.</summary>\n",
    "\n",
    "```python\n",
    "with simulated_robot:\n",
    "    poseHard = Pose([1.3, 2.7, 0], [0, 0, 1, 0])\n",
    "    NavigateAction([poseHard]).resolve().perform()\n",
    "\n",
    "    ParkArmsAction([Arms.BOTH]).resolve().perform()\n",
    "\n",
    "    MoveTorsoAction([TorsoState.HIGH]).resolve().perform()\n",
    "    handle_desig = ObjectPart(names=[\"handle_cab3_door_top\"], part_of=apartment_desig.resolve())\n",
    "    closed_location, opened_location = AccessingLocation(handle_desig=handle_desig.resolve(),\n",
    "                                                         robot_desig=robot_desig.resolve()).resolve()\n",
    "    OpenAction(object_designator_description=handle_desig, arms=[closed_location.arms[0]],\n",
    "               start_goal_location=[closed_location, opened_location]).resolve().perform()\n",
    "\n",
    "    LookAtAction(targets=[milk_desig.resolve().pose]).resolve().perform()\n",
    "    obj_desig = DetectAction(milk_desig).resolve().perform()\n",
    "    milk_target_pose = Pose([5.34, 3.55, 0.8])\n",
    "    ParkArmsAction([Arms.BOTH]).resolve().perform()\n",
    "    MoveTorsoAction([TorsoState.HIGH]).resolve().perform()\n",
    "     robot_desig = BelieveObject(names=[RobotDescription.current_robot_description.name])\n",
    "        ParkArmsActionPerformable(Arms.BOTH).perform()\n",
    "\n",
    "        if self.object_designator.obj_type == ObjectType.BOWL or self.object_designator.obj_type == ObjectType.SPOON:\n",
    "            grasp = calculate_object_faces(self.object_designator)[1]\n",
    "        else:\n",
    "            grasp = calculate_object_faces(self.object_designator)[0]\n",
    "\n",
    "        pickup_loc = CostmapLocation(\n",
    "            target=self.object_designator,\n",
    "            reachable_for=robot_desig.resolve(),\n",
    "            reachable_arm=self.arm,\n",
    "            used_grasps=[grasp]\n",
    "        )\n",
    "\n",
    "        # Tries to find a pick-up posotion for the robot that uses the given arm\n",
    "        pickup_pose = next((pose for pose in pickup_loc if self.arm in pose.reachable_arms), None)\n",
    "        if not pickup_pose:\n",
    "            raise ObjectUnfetchable(\n",
    "                f\"No reachable pose found for the robot to grasp the object: {self.object_designator} with arm: {self.arm}\"\n",
    "            )\n",
    "\n",
    "        NavigateActionPerformable(pickup_pose.pose).perform()\n",
    "        PickUpActionPerformable(self.object_designator, self.arm, grasp).perform()\n",
    "        ParkArmsActionPerformable(Arms.BOTH).perform()\n",
    "        try:\n",
    "            place_loc = CostmapLocation(\n",
    "                target=self.target_location,\n",
    "                reachable_for=robot_desig.resolve(),\n",
    "                reachable_arm=self.arm,\n",
    "                used_grasps=[grasp],\n",
    "                object_in_hand=self.object_designator\n",
    "            ).resolve()\n",
    "        except StopIteration:\n",
    "            raise ReachabilityFailure(\n",
    "                f\"No reachable location found for the target location: {self.target_location}\"\n",
    "            )\n",
    "        NavigateActionPerformable(place_loc.pose).perform()\n",
    "        PlaceActionPerformable(self.object_designator, self.arm, self.target_location).perform()\n",
    "        ParkArmsActionPerformable(Arms.BOTH).perform()\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c1d2ff7e1f6fc",
   "metadata": {},
   "source": [
    "Now we have our first complete transport plan! While this is a solid foundation for understanding the basics, it lacks some advanced cognitive capabilities.\n",
    "\n",
    "### What’s Missing?\n",
    "\n",
    "- **Failure Handling**: Adding failure handling is crucial. It not only makes the robot more resilient but also highlights the complexities a robot control system must navigate. Designing a generalized failure-handling approach requires the control program to be semantically transparent, allowing it to retry, replan, transform, or adapt to different types of failures without hidden information.\n",
    "\n",
    "- **Learning from Experience**: Another concept is memory-based learning. By logging each action the robot performs, we can replay and analyze this data to help the robot identify and correct mistakes. You’ll find more on this in Chapter 6’s extra materials.\n",
    "\n",
    "- **Enhanced Physical Understanding**: In this simulation, the robot's physical interactions are simplified, which bypasses some real-world challenges. Implementing true physical understanding would involve additional considerations beyond what’s covered here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
